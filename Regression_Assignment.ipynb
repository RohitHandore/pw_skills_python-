{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
        "example of each."
      ],
      "metadata": {
        "id": "OzMFQegJgAma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Linear Regression**:\n",
        "\n",
        "Simple linear regression is a statistical method used to model the relationship between a single independent variable \\( x \\) and a dependent variable \\( y \\). It assumes that the relationship between the variables can be represented by a straight line.\n",
        "\n",
        "**Equation**:\n",
        "\\[ y = \\beta_0 + \\beta_1 x + \\varepsilon \\]\n",
        "\n",
        "- \\( y \\) is the dependent variable.\n",
        "- \\( x \\) is the independent variable.\n",
        "- \\( \\beta_0 \\) is the intercept (the value of \\( y \\) when \\( x = 0 \\)).\n",
        "- \\( \\beta_1 \\) is the slope (the change in \\( y \\) for a one-unit change in \\( x \\)).\n",
        "- \\( \\varepsilon \\) is the error term.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "Suppose we want to predict students' scores (\\( y \\)) based on the number of hours they study (\\( x \\)). In this case, the number of hours studied is the independent variable, and the scores achieved are the dependent variable.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "hours_studied = np.array([3, 4, 5, 6, 7, 8]).reshape(-1, 1)  # Independent variable\n",
        "scores = np.array([60, 70, 75, 80, 85, 90])  # Dependent variable\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(hours_studied, scores)\n",
        "\n",
        "# Plot the data and the regression line\n",
        "plt.scatter(hours_studied, scores, color='blue')\n",
        "plt.plot(hours_studied, model.predict(hours_studied), color='red')\n",
        "plt.xlabel('Hours Studied')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Simple Linear Regression')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Multiple Linear Regression**:\n",
        "\n",
        "Multiple linear regression is an extension of simple linear regression where there are two or more independent variables. It models the relationship between multiple independent variables \\( x_1, x_2, \\ldots, x_n \\) and a dependent variable \\( y \\).\n",
        "\n",
        "**Equation**:\n",
        "\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon \\]\n",
        "\n",
        "- \\( x_1, x_2, \\ldots, x_n \\) are the independent variables.\n",
        "- \\( \\beta_0 \\) is the intercept.\n",
        "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients for each independent variable.\n",
        "- \\( \\varepsilon \\) is the error term.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "Suppose we want to predict house prices (\\( y \\)) based on the size of the house (\\( x_1 \\)) and the number of bedrooms (\\( x_2 \\)).\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "sizes = np.array([1000, 1500, 2000, 2500, 3000]).reshape(-1, 1)  # Independent variable 1\n",
        "bedrooms = np.array([2, 3, 3, 4, 4]).reshape(-1, 1)  # Independent variable 2\n",
        "prices = np.array([300000, 400000, 500000, 600000, 700000])  # Dependent variable\n",
        "\n",
        "# Fit the model\n",
        "X = np.column_stack((sizes, bedrooms))\n",
        "model = LinearRegression()\n",
        "model.fit(X, prices)\n",
        "\n",
        "# Plot the data and the regression plane\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(sizes, bedrooms, prices, color='blue')\n",
        "x1, x2 = np.meshgrid(np.linspace(sizes.min(), sizes.max(), 10), np.linspace(bedrooms.min(), bedrooms.max(), 10))\n",
        "y_pred = model.predict(np.column_stack((x1.flatten(), x2.flatten())))\n",
        "ax.plot_surface(x1, x2, y_pred.reshape(x1.shape), alpha=0.5, color='red')\n",
        "ax.set_xlabel('Size')\n",
        "ax.set_ylabel('Bedrooms')\n",
        "ax.set_zlabel('Price')\n",
        "ax.set_title('Multiple Linear Regression')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "In this example, the size of the house and the number of bedrooms are the independent variables, and the house price is the dependent variable."
      ],
      "metadata": {
        "id": "Laojbx0sgM8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
        "a given dataset?"
      ],
      "metadata": {
        "id": "GgqhbQ2IgXtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression makes several assumptions about the data:\n",
        "\n",
        "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
        "   \n",
        "2. **Independence of Errors**: The errors (residuals) are independent of each other. There should be no correlation between consecutive errors.\n",
        "\n",
        "3. **Homoscedasticity**: The variance of the errors is constant across all levels of the independent variables. In other words, the spread of the residuals is the same for all values of the independent variables.\n",
        "\n",
        "4. **Normality of Errors**: The errors are normally distributed. This assumption ensures that the parameter estimates are unbiased and have minimum variance.\n",
        "\n",
        "5. **No Perfect Multicollinearity**: There is no perfect multicollinearity among the independent variables. Perfect multicollinearity occurs when one independent variable can be exactly predicted from others.\n",
        "\n",
        "### Checking Assumptions:\n",
        "\n",
        "1. **Linearity**:\n",
        "   - Plot the independent variable against the dependent variable. If the relationship looks approximately linear, the assumption holds.\n",
        "   - Use residual plots or partial regression plots to check linearity.\n",
        "\n",
        "2. **Independence of Errors**:\n",
        "   - Plot residuals against the order of observation (time or data collection sequence). There should be no pattern or trend in the residuals.\n",
        "   - Use autocorrelation plots to check for correlation between consecutive residuals.\n",
        "\n",
        "3. **Homoscedasticity**:\n",
        "   - Plot residuals against the predicted values. The spread of residuals should be roughly constant across all predicted values.\n",
        "   - Use a residual vs. fitted plot to check for heteroscedasticity.\n",
        "\n",
        "4. **Normality of Errors**:\n",
        "   - Plot a histogram or a Q-Q plot of the residuals. They should follow a roughly normal distribution.\n",
        "   - Use statistical tests like the Shapiro-Wilk test or the Kolmogorov-Smirnov test to assess normality.\n",
        "\n",
        "5. **No Perfect Multicollinearity**:\n",
        "   - Calculate the correlation matrix for the independent variables. Look for high correlations (close to ±1) between variables.\n",
        "   - Use variance inflation factors (VIF) to quantify multicollinearity.\n",
        "\n",
        "### Example Code (Python):\n",
        "\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Fit the linear regression model\n",
        "X = sm.add_constant(X)  # Add constant for intercept\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Check assumptions\n",
        "# 1. Linearity\n",
        "plt.scatter(X[:, 1], y)  # Check linearity between first independent variable and dependent variable\n",
        "plt.xlabel('Independent Variable')\n",
        "plt.ylabel('Dependent Variable')\n",
        "plt.title('Linearity Check')\n",
        "plt.show()\n",
        "\n",
        "# 2. Independence of Errors\n",
        "plt.plot(model.resid)  # Check for patterns or trends in residuals\n",
        "plt.xlabel('Observation')\n",
        "plt.ylabel('Residual')\n",
        "plt.title('Independence of Errors Check')\n",
        "plt.show()\n",
        "\n",
        "# 3. Homoscedasticity\n",
        "plt.scatter(model.predict(), model.resid)  # Check spread of residuals vs. predicted values\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Homoscedasticity Check')\n",
        "plt.show()\n",
        "\n",
        "# 4. Normality of Errors\n",
        "sm.qqplot(model.resid, line='45')  # Q-Q plot of residuals\n",
        "plt.title('Normality of Errors Check')\n",
        "plt.show()\n",
        "\n",
        "# 5. No Perfect Multicollinearity\n",
        "vif = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
        "print(\"VIF:\", vif)\n",
        "```\n",
        "\n",
        "This code checks the assumptions of linear regression using Python's statsmodels library. Adjust the plots and tests based on the specific assumptions you want to check."
      ],
      "metadata": {
        "id": "7Ny8uV6DgcBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
        "a real-world scenario."
      ],
      "metadata": {
        "id": "SCmbgEOVgk29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a linear regression model of the form \\( y = \\beta_0 + \\beta_1 x + \\varepsilon \\), the slope (\\( \\beta_1 \\)) represents the change in the dependent variable (\\( y \\)) for a one-unit change in the independent variable (\\( x \\)). The intercept (\\( \\beta_0 \\)) represents the value of the dependent variable (\\( y \\)) when the independent variable (\\( x \\)) is zero. Here's how to interpret them:\n",
        "\n",
        "- **Intercept (\\( \\beta_0 \\))**: It represents the value of the dependent variable when all independent variables are zero. It is the predicted value of \\( y \\) when \\( x = 0 \\) (although this may not always be meaningful depending on the context). For example, if \\( \\beta_0 = 10 \\), it means that when \\( x = 0 \\), the expected value of \\( y \\) is 10.\n",
        "\n",
        "- **Slope (\\( \\beta_1 \\))**: It represents the change in the dependent variable for a one-unit change in the independent variable, assuming all other variables remain constant. For example, if \\( \\beta_1 = 3 \\), it means that for every one-unit increase in \\( x \\), \\( y \\) is expected to increase by 3 units.\n",
        "\n",
        "### Example:\n",
        "\n",
        "**Scenario**:\n",
        "Suppose we want to predict the salary (\\( y \\)) of employees based on their years of experience (\\( x \\)). We have a dataset where \\( x \\) represents years of experience, and \\( y \\) represents salary.\n",
        "\n",
        "**Interpretation**:\n",
        "- **Intercept (\\( \\beta_0 \\))**: If the intercept (\\( \\beta_0 \\)) is $30,000, it means that an employee with zero years of experience is expected to have a salary of $30,000.\n",
        "- **Slope (\\( \\beta_1 \\))**: If the slope (\\( \\beta_1 \\)) is 3,000, it means that for each additional year of experience, the salary is expected to increase by $3,000.\n",
        "\n",
        "**Example Code**:\n",
        "```python\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample data\n",
        "years_experience = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
        "salary = np.array([33000, 36000, 39000, 42000, 45000])  # Dependent variable\n",
        "\n",
        "# Add constant for intercept\n",
        "X = sm.add_constant(years_experience)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(salary, X).fit()\n",
        "\n",
        "# Print the intercept and slope\n",
        "print(\"Intercept (β0):\", model.params[0])\n",
        "print(\"Slope (β1):\", model.params[1])\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "```\n",
        "Intercept (β0): 30000.0\n",
        "Slope (β1): 3000.0\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The intercept (\\( \\beta_0 \\)) of $30,000 means that an employee with zero years of experience is expected to have a salary of $30,000.\n",
        "- The slope (\\( \\beta_1 \\)) of $3,000 means that for each additional year of experience, the salary is expected to increase by $3,000."
      ],
      "metadata": {
        "id": "lYzV4bTzgqJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
      ],
      "metadata": {
        "id": "OjSdr9eMg3tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent** is an optimization algorithm used to minimize the cost function (or loss function) of a model by iteratively adjusting the model parameters. It is a first-order optimization algorithm that finds the minimum of a function by taking steps proportional to the negative of the gradient of the function at the current point. In simple terms, it moves in the direction opposite to the gradient to reach the minimum point.\n",
        "\n",
        "### Working of Gradient Descent:\n",
        "\n",
        "1. **Initialization**: Start with an initial guess for the model parameters (weights).\n",
        "\n",
        "2. **Compute Gradient**: Compute the gradient of the cost function with respect to each parameter. The gradient indicates the direction of steepest ascent.\n",
        "\n",
        "3. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient to minimize the cost function.\n",
        "\n",
        "4. **Iterate**: Repeat steps 2 and 3 until convergence (i.e., until the change in the cost function is negligible or after a fixed number of iterations).\n",
        "\n",
        "### Types of Gradient Descent:\n",
        "\n",
        "1. **Batch Gradient Descent**: It calculates the gradient using the entire dataset. It requires the entire dataset to be loaded into memory, making it computationally expensive for large datasets.\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD)**: It updates the parameters using only one random data point at a time. It is computationally less expensive but can be noisy due to frequent updates.\n",
        "\n",
        "3. **Mini-Batch Gradient Descent**: It updates the parameters using a small random subset (mini-batch) of the dataset. It combines the advantages of both batch and stochastic gradient descent.\n",
        "\n",
        "### Use in Machine Learning:\n",
        "\n",
        "- **Model Training**: Gradient descent is used to train various machine learning models, such as linear regression, logistic regression, neural networks, and support vector machines.\n",
        "\n",
        "- **Optimization**: It is used to optimize the parameters (weights and biases) of the model to minimize the error (cost function) between predicted and actual values.\n",
        "\n",
        "- **Feature Selection**: It can be used for feature selection by penalizing the coefficients of less important features, leading to their elimination.\n",
        "\n",
        "- **Hyperparameter Tuning**: Gradient descent can also be used to tune hyperparameters such as learning rate and regularization parameters.\n",
        "\n",
        "### Example:\n",
        "\n",
        "In linear regression, the cost function (mean squared error) is defined as:\n",
        "\\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\n",
        "\n",
        "Where:\n",
        "- \\( J(\\theta) \\) is the cost function.\n",
        "- \\( \\theta \\) are the model parameters (weights).\n",
        "- \\( h_\\theta(x^{(i)}) \\) is the predicted value.\n",
        "- \\( y^{(i)} \\) is the actual value.\n",
        "- \\( m \\) is the number of training examples.\n",
        "\n",
        "Gradient descent updates the parameters as:\n",
        "\\[ \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\]\n",
        "\n",
        "Where:\n",
        "- \\( \\alpha \\) is the learning rate.\n",
        "- \\( x_j^{(i)} \\) is the value of feature \\( j \\) in the \\( i \\)th training example.\n",
        "\n",
        "In each iteration, the parameters are adjusted in the direction that reduces the cost function until convergence."
      ],
      "metadata": {
        "id": "WxvGuvWkg86w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
      ],
      "metadata": {
        "id": "1CXuAbsUheXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Linear Regression** is a statistical technique used to model the relationship between multiple independent variables and a single dependent variable. In multiple linear regression, the dependent variable \\( y \\) is assumed to be a linear function of two or more independent variables \\( x_1, x_2, \\ldots, x_n \\), along with an error term \\( \\varepsilon \\).\n",
        "\n",
        "### Equation:\n",
        "\n",
        "The equation for multiple linear regression is:\n",
        "\n",
        "\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon \\]\n",
        "\n",
        "- \\( y \\) is the dependent variable.\n",
        "- \\( x_1, x_2, \\ldots, x_n \\) are the independent variables.\n",
        "- \\( \\beta_0 \\) is the intercept (the value of \\( y \\) when all independent variables are zero).\n",
        "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients for each independent variable.\n",
        "- \\( \\varepsilon \\) is the error term.\n",
        "\n",
        "### Differences from Simple Linear Regression:\n",
        "\n",
        "1. **Number of Independent Variables**:\n",
        "   - In simple linear regression, there is only one independent variable \\( x \\).\n",
        "   - In multiple linear regression, there are two or more independent variables \\( x_1, x_2, \\ldots, x_n \\).\n",
        "\n",
        "2. **Equation**:\n",
        "   - Simple linear regression equation: \\( y = \\beta_0 + \\beta_1 x + \\varepsilon \\)\n",
        "   - Multiple linear regression equation: \\( y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon \\)\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - In simple linear regression, the slope represents the change in \\( y \\) for a one-unit change in \\( x \\).\n",
        "   - In multiple linear regression, each coefficient (\\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\)) represents the change in \\( y \\) for a one-unit change in the corresponding independent variable, holding other variables constant.\n",
        "\n",
        "4. **Model Complexity**:\n",
        "   - Multiple linear regression models are more complex than simple linear regression models as they account for the effects of multiple variables on the dependent variable.\n",
        "\n",
        "### Example:\n",
        "\n",
        "**Scenario**:\n",
        "Suppose we want to predict the price of a house (\\( y \\)) based on its size (\\( x_1 \\)) and the number of bedrooms (\\( x_2 \\)).\n",
        "\n",
        "**Multiple Linear Regression Equation**:\n",
        "\\[ \\text{Price} = \\beta_0 + \\beta_1 \\times \\text{Size} + \\beta_2 \\times \\text{Bedrooms} + \\varepsilon \\]\n",
        "\n",
        "- \\( \\beta_0 \\): Intercept (price when both size and bedrooms are zero).\n",
        "- \\( \\beta_1 \\): The effect of size on price, holding bedrooms constant.\n",
        "- \\( \\beta_2 \\): The effect of bedrooms on price, holding size constant.\n",
        "\n",
        "In this example, we consider both size and bedrooms as independent variables, unlike in simple linear regression where we would consider only one variable (e.g., size).\n",
        "\n",
        "**Example Code**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1000, 2], [1500, 3], [2000, 3], [2500, 4], [3000, 4]])  # Independent variables (size, bedrooms)\n",
        "y = np.array([300000, 400000, 500000, 600000, 700000])  # Dependent variable (price)\n",
        "\n",
        "# Add constant for intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the multiple linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())\n",
        "```\n",
        "\n",
        "This code fits a multiple linear regression model using Python's statsmodels library and prints the model summary, which includes the coefficients (intercept, size, and bedrooms) and other statistics."
      ],
      "metadata": {
        "id": "2LAq4NVFhn7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
        "address this issue?"
      ],
      "metadata": {
        "id": "e3Aub06xh0oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multicollinearity** refers to the situation where two or more independent variables in a multiple linear regression model are highly correlated with each other. This can cause issues in the model estimation because it becomes difficult for the model to differentiate the individual effects of the correlated variables on the dependent variable.\n",
        "\n",
        "### Detecting Multicollinearity:\n",
        "\n",
        "1. **Correlation Matrix**: Calculate the correlation matrix between independent variables. High correlation coefficients (close to ±1) indicate multicollinearity.\n",
        "\n",
        "2. **Variance Inflation Factor (VIF)**: Compute the VIF for each independent variable. VIF measures how much the variance of an estimated coefficient is inflated due to multicollinearity. High VIF values (typically > 10) indicate multicollinearity.\n",
        "\n",
        "### Addressing Multicollinearity:\n",
        "\n",
        "1. **Remove Highly Correlated Variables**: If two or more independent variables are highly correlated, consider removing one of them from the model.\n",
        "\n",
        "2. **Feature Selection**: Use feature selection techniques (e.g., forward selection, backward elimination, or stepwise regression) to choose a subset of independent variables that are most relevant to the dependent variable.\n",
        "\n",
        "3. **Principal Component Analysis (PCA)**: Use PCA to transform the correlated variables into a set of uncorrelated variables (principal components) and then use these components in the regression model.\n",
        "\n",
        "4. **Regularization**: Regularization techniques like Ridge Regression and Lasso Regression can help mitigate multicollinearity by penalizing large coefficients.\n",
        "\n",
        "### Example Code for Detecting Multicollinearity (VIF):\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'x1': [1, 2, 3, 4, 5],\n",
        "    'x2': [2, 4, 6, 8, 10],\n",
        "    'x3': [3, 6, 9, 12, 15]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate VIF\n",
        "X = df.values\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = df.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif)\n",
        "```\n",
        "\n",
        "This code calculates the VIF for each independent variable in the dataset. If any variable has a VIF greater than a certain threshold (e.g., 10), it indicates multicollinearity."
      ],
      "metadata": {
        "id": "BKYzGZNDh4B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Describe the polynomial regression model. How is it different from linear regression?"
      ],
      "metadata": {
        "id": "DuMVmtIJiFdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Polynomial Regression** is a form of regression analysis in which the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)th degree polynomial in \\(x\\). Unlike linear regression, which fits a straight line to the data, polynomial regression fits a curve to the data.\n",
        "\n",
        "### Equation:\n",
        "\n",
        "The general equation for polynomial regression of degree \\(n\\) is:\n",
        "\n",
        "\\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_n x^n + \\varepsilon \\]\n",
        "\n",
        "- \\( y \\) is the dependent variable.\n",
        "- \\( x \\) is the independent variable.\n",
        "- \\( \\beta_0, \\beta_1, \\ldots, \\beta_n \\) are the coefficients.\n",
        "- \\( \\varepsilon \\) is the error term.\n",
        "\n",
        "### Differences from Linear Regression:\n",
        "\n",
        "1. **Model Complexity**:\n",
        "   - Polynomial regression can capture non-linear relationships between the independent and dependent variables. It is more flexible than linear regression, which assumes a linear relationship.\n",
        "\n",
        "2. **Equation**:\n",
        "   - Linear regression equation: \\( y = \\beta_0 + \\beta_1 x + \\varepsilon \\)\n",
        "   - Polynomial regression equation: \\( y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_n x^n + \\varepsilon \\)\n",
        "\n",
        "3. **Curve Fitting**:\n",
        "   - Linear regression fits a straight line to the data.\n",
        "   - Polynomial regression fits a curve to the data, allowing for more complex patterns to be captured.\n",
        "\n",
        "4. **Interpretation**:\n",
        "   - In linear regression, the coefficients represent the change in the dependent variable for a one-unit change in the independent variable.\n",
        "   - In polynomial regression, the interpretation of coefficients becomes more complex as they represent the change in \\( y \\) for a change in \\( x \\), \\( x^2 \\), \\( x^3 \\), and so on.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Suppose we have data on the temperature (\\( x \\)) and the rate of ice cream sales (\\( y \\)). We want to model the relationship between temperature and ice cream sales using polynomial regression.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([20, 25, 30, 35, 40, 45]).reshape(-1, 1)  # Temperature\n",
        "y = np.array([100, 120, 150, 180, 220, 250])  # Ice cream sales\n",
        "\n",
        "# Transform features to include polynomial terms up to degree 2\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Plot data and the polynomial curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.xlabel('Temperature')\n",
        "plt.ylabel('Ice Cream Sales')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "In this example, the polynomial regression model will fit a curve to the data instead of a straight line. This allows for capturing the non-linear relationship between temperature and ice cream sales more accurately compared to linear regression."
      ],
      "metadata": {
        "id": "9SaKpDD0iJ4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
        "regression? In what situations would you prefer to use polynomial regression?"
      ],
      "metadata": {
        "id": "fQusAO5ciPdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Polynomial Regression:**\n",
        "\n",
        "1. **Flexibility**: Polynomial regression can capture non-linear relationships between the independent and dependent variables, allowing for more complex patterns to be modeled.\n",
        "\n",
        "2. **Improved Accuracy**: By fitting a curve to the data instead of a straight line, polynomial regression can provide a better fit for datasets with non-linear relationships.\n",
        "\n",
        "3. **No Assumption of Linearity**: Unlike linear regression, polynomial regression does not assume a linear relationship between the independent and dependent variables.\n",
        "\n",
        "**Disadvantages of Polynomial Regression:**\n",
        "\n",
        "1. **Overfitting**: Using higher degree polynomials can lead to overfitting, where the model learns the noise in the data rather than the underlying pattern. This can result in poor generalization to new data.\n",
        "\n",
        "2. **Interpretability**: As the degree of the polynomial increases, the interpretation of coefficients becomes more complex and less intuitive.\n",
        "\n",
        "3. **Increased Complexity**: Polynomial regression models are more complex than linear regression models, making them computationally more intensive and harder to interpret.\n",
        "\n",
        "**When to Use Polynomial Regression:**\n",
        "\n",
        "1. **Non-linear Relationships**: When the relationship between the independent and dependent variables is non-linear, polynomial regression can provide a better fit to the data than linear regression.\n",
        "\n",
        "2. **Curvilinear Relationships**: When the relationship between the variables follows a curve (e.g., quadratic, cubic), polynomial regression can accurately capture this curvature.\n",
        "\n",
        "3. **Limited Data**: In cases where the dataset is limited and linear regression does not provide a good fit, polynomial regression can help capture more complex patterns in the data.\n",
        "\n",
        "4. **Predictive Accuracy**: When the goal is to maximize predictive accuracy and linear regression does not sufficiently capture the underlying pattern in the data.\n",
        "\n",
        "5. **Domain Knowledge**: When domain knowledge suggests a non-linear relationship between the variables, polynomial regression may be preferred.\n",
        "\n",
        "However, it's important to be cautious when using polynomial regression, especially with higher degree polynomials, as it can easily lead to overfitting. Regularization techniques like Ridge Regression or Lasso Regression can help mitigate this issue. Additionally, cross-validation can be used to evaluate the performance of the model and prevent overfitting."
      ],
      "metadata": {
        "id": "tmNVeaWXiUwJ"
      }
    }
  ]
}