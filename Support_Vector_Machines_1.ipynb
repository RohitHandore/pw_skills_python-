{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the mathematical formula for a linear SVM?\n",
        "Q2. What is the objective function of a linear SVM?\n",
        "Q3. What is the kernel trick in SVM?\n",
        "Q4. What is the role of support vectors in SVM Explain with example\n",
        "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
        "SVM?\n",
        "Q6. SVM Implementation through Iris dataset.\n",
        "\n",
        "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
        "performance with the scikit-learn implementation.\n",
        "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
        "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
        "~ Compute the accuracy of the model on the testing setl\n",
        "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
        "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
        "the model."
      ],
      "metadata": {
        "id": "N7COJV_MCn-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers:\n",
        "\n",
        "Q1. The mathematical formula for a linear SVM is:\n",
        "\n",
        "w^T x + b = 0\n",
        "\n",
        "where w is the weight vector, x is the input vector, and b is the bias term.\n",
        "\n",
        "Q2. The objective function of a linear SVM is to maximize the margin between the classes, which can be written as:\n",
        "\n",
        "Maximize: 2/||w||^2\n",
        "\n",
        "Subject to: y_i (w^T x_i + b) >= 1 for all i\n",
        "\n",
        "where y_i is the label of the i-th data point, x_i is the i-th data point, and ||w|| is the Euclidean norm of the weight vector.\n",
        "\n",
        "Q3. The kernel trick in SVM is a method to transform the data into a higher-dimensional space where the classes are linearly separable. This is done by applying a kernel function to the data, which maps the data from the original space to the higher-dimensional space.\n",
        "\n",
        "Q4. Support vectors are the data points that lie closest to the decision boundary (hyperplane) and play a crucial role in defining the margin. They are the most informative data points for the classifier.\n",
        "\n",
        "Example: Suppose we have a dataset of two classes, represented by red and blue points. The support vectors are the points that lie closest to the decision boundary (hyperplane).\n",
        "\n",
        "Q5. Here are the illustrations of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM:\n",
        "\n",
        "- Hyperplane: The decision boundary that separates the classes.\n",
        "- Marginal plane: The plane that passes through the support vectors and is parallel to the hyperplane.\n",
        "- Soft margin: The region between the hyperplane and the marginal plane, where the classifier allows for some misclassifications.\n",
        "- Hard margin: The hyperplane that separates the classes with no misclassifications.\n",
        "\n",
        "Graphs:\n",
        "\n",
        "- Hyperplane: A straight line that separates the classes.\n",
        "- Marginal plane: A parallel line to the hyperplane that passes through the support vectors.\n",
        "- Soft margin: A shaded region between the hyperplane and the marginal plane.\n",
        "- Hard margin: A straight line that separates the classes with no misclassifications.\n",
        "\n",
        "Q6. Here is the implementation of a linear SVM classifier from scratch using Python:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into a training set and a testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the linear SVM classifier\n",
        "def linear_svm(X, y):\n",
        "\t# Compute the weight vector and bias term\n",
        "\tw = np.zeros(X.shape[1])\n",
        "\tb = 0\n",
        "\tfor i in range(X.shape[0]):\n",
        "\t\tif y[i] == 1:\n",
        "\t\t\tw += X[i]\n",
        "\t\telse:\n",
        "\t\t\tw -= X[i]\n",
        "\tb = -np.mean(w)\n",
        "\n",
        "\t# Return the weight vector and bias term\n",
        "\treturn w, b\n",
        "\n",
        "# Train the linear SVM classifier on the training set\n",
        "w, b = linear_svm(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the testing set\n",
        "y_pred = np.sign(np.dot(X_test, w) + b)\n",
        "\n",
        "# Compute the accuracy of the model on the testing set\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Plot the decision boundaries of the trained model using two of the features\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
        "x_min, x_max = X_test[:, 0].min(), X_test[:, 0].max()\n",
        "y_min, y_max = X_test[:, 1].min(), X_test[:, 1].max()\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))\n",
        "Z = np.sign(np.dot(np.c_[xx.ravel(), yy.ravel()], w) + b)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contour(xx, yy, Z, alpha=0.8)\n",
        "plt.show()\n",
        "\n",
        "# Try different values of the regularisation parameter C and see how it affects the performance of the model\n",
        "C_values = [0.1, 1, 10]\n",
        "for C in C_values:\n",
        "\tw, b = linear_svm(X_train, y_train, C=C)\n",
        "\ty_pred = np.sign(np.dot(X_test, w) + b)\n",
        "\taccuracy = np.mean(y_pred == y_test)\n",
        "\tprint(\"Accuracy for C =\", C, \":\", accuracy)\n",
        "\n",
        "Note: This implementation is a simplified version of the linear SVM classifier and does not include regularization. The scikit-learn implementation includes regularization and other features that are not included in this implementation.\n",
        "\n",
        "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its performance with the scikit-learn implementation."
      ],
      "metadata": {
        "id": "GbgyNCmrCuO9"
      }
    }
  ]
}