{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?\n",
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
        "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
        "performance using metrics such as mean squared error and R-squared.\n",
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
        "optimise the performance of the model. Use grid search or random search to find the best\n",
        "hyperparameters\n",
        "Q4. What is a weak learner in Gradient Boosting?\n",
        "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
        "algorithm?"
      ],
      "metadata": {
        "id": "s-siK25UyVzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers to your questions:\n",
        "\n",
        "1. Gradient Boosting Regression is a supervised learning algorithm used for regression tasks. It combines multiple weak learners to create a strong learner, with each learner attempting to correct the errors of the previous learner.\n",
        "\n",
        "2. Here is a simple implementation of Gradient Boosting Regression from scratch using Python and NumPy:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "\tdef __init__(self, learning_rate=0.1, n_estimators=100, max_depth=3):\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.n_estimators = n_estimators\n",
        "\t\tself.max_depth = max_depth\n",
        "\t\tself.trees = []\n",
        "\n",
        "\tdef fit(self, X, y):\n",
        "\t\tself.trees = []\n",
        "\t\tfor _ in range(self.n_estimators):\n",
        "\t\t\ttree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "\t\t\ttree.fit(X, y)\n",
        "\t\t\tself.trees.append(tree)\n",
        "\t\t\ty_pred = tree.predict(X)\n",
        "\t\t\ty_residual = y - y_pred\n",
        "\t\t\tX_residual = X\n",
        "\t\t\tfor tree in self.trees:\n",
        "\t\t\t\ttree.fit(X_residual, y_residual)\n",
        "\t\t\t\ty_pred = tree.predict(X_residual)\n",
        "\t\t\t\ty_residual = y_residual - y_pred\n",
        "\t\t\t\tX_residual = X_residual\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\ty_pred = np.zeros(X.shape[0])\n",
        "\t\tfor tree in self.trees:\n",
        "\t\t\ty_pred += tree.predict(X)\n",
        "\t\treturn y_pred\n",
        "\n",
        "This implementation uses Decision Trees as the weak learners and trains each tree on the residuals of the previous tree.\n",
        "\n",
        "1. To optimize the performance of the model, you can experiment with different hyperparameters such as learning rate, number of trees, and tree depth using grid search or random search. For example:\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 1], 'n_estimators': [10, 50, 100], 'max_depth': [1, 3, 5]}\n",
        "grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X, y)\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n",
        "\n",
        "This code performs a grid search over the specified hyperparameters and prints the best combination of hyperparameters and the corresponding score (mean squared error).\n",
        "\n",
        "1. A weak learner in Gradient Boosting is a model that is trained to correct the errors of the previous learner. In the context of regression, a weak learner is a model that predicts the residuals of the previous learner.\n",
        "\n",
        "2. The intuition behind Gradient Boosting is that each learner attempts to correct the errors of the previous learner, with the goal of improving the overall accuracy of the model. By combining multiple weak learners, Gradient Boosting can create a strong learner that generalizes well to new data.\n",
        "\n",
        "3. Gradient Boosting builds an ensemble of weak learners by iteratively training each learner on the residuals of the previous learner. Each learner is trained to correct the errors of the previous learner, with the goal of improving the overall accuracy of the model.\n",
        "\n",
        "4. The steps involved in constructing the mathematical intuition of Gradient Boosting are:\n",
        "- Define the loss function (e.g. mean squared error)\n",
        "- Define the weak learner (e.g. Decision Tree)\n",
        "- Define the ensemble method (e.g. gradient descent)\n",
        "- Derive the update rule for each learner\n",
        "- Prove the convergence of the algorithm\n",
        "\n",
        "Note: Gradient Boosting is a powerful algorithm for regression tasks, and its performance can be optimized by tuning hyperparameters and using techniques such as grid search and random search."
      ],
      "metadata": {
        "id": "iqj_V23yyY3K"
      }
    }
  ]
}