{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
        "model's performance?\n",
        "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
      ],
      "metadata": {
        "id": "fi3SClRDH7hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers:\n",
        "\n",
        "Q1. Lasso Regression is a type of linear regression that uses L1 regularization to reduce overfitting by adding a penalty term to the loss function. This penalty term is proportional to the absolute value of the model's coefficients, which encourages the model to have smaller coefficients. Lasso Regression differs from other regression techniques in its use of L1 regularization, which leads to sparse models with some coefficients set to zero.\n",
        "\n",
        "Q2. The main advantage of using Lasso Regression in feature selection is that it automatically selects the most relevant features by setting the coefficients of the less relevant features to zero. This is known as sparse modeling.\n",
        "\n",
        "Q3. The coefficients of a Lasso Regression model can be interpreted similarly to those of a linear regression model, but with the added consideration that some coefficients may be set to zero due to the L1 regularization.\n",
        "\n",
        "Q4. The tuning parameters that can be adjusted in Lasso Regression are the regularization parameter (lambda) and the maximum number of iterations. The regularization parameter controls the strength of the penalty term, and the maximum number of iterations controls the convergence of the algorithm. A higher value of lambda leads to a stronger penalty and more coefficients set to zero, while a lower value of lambda leads to a weaker penalty and more coefficients retained.\n",
        "\n",
        "Q5. Yes, Lasso Regression can be used for non-linear regression problems by using non-linear features or basis expansions, such as polynomial or spline features.\n",
        "\n",
        "Q6. Ridge Regression and Lasso Regression are both regularization techniques, but they use different types of regularization. Ridge Regression uses L2 regularization, which adds a penalty term proportional to the square of the coefficients, while Lasso Regression uses L1 regularization, which adds a penalty term proportional to the absolute value of the coefficients. Ridge Regression leads to a model with smaller coefficients, while Lasso Regression leads to a sparse model with some coefficients set to zero.\n",
        "\n",
        "Q7. Yes, Lasso Regression can handle multicollinearity in the input features by setting the coefficients of the highly correlated features to zero.\n",
        "\n",
        "Q8. The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using cross-validation, grid search, or other hyperparameter tuning techniques. The goal is to find the value of lambda that leads to the best trade-off between model complexity and model accuracy."
      ],
      "metadata": {
        "id": "A8aXRDLWJ_QW"
      }
    }
  ]
}