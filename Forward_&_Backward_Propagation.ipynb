{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the purpose of forward propagation in a neural network?\n",
        "\n",
        "Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?\n",
        "\n",
        "Q3. How are activation functions used during forward propagation?\n",
        "\n",
        "Q4. What is the role of weights and biases in forward propagation?\n",
        "\n",
        "Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?\n",
        "\n",
        "Q6. What is the purpose of backward propagation in a neural network?\n",
        "\n",
        "Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?\n",
        "\n",
        "Q8. Can you explain the concept of the chain rule and its application in backward propagation?\n",
        "\n",
        "Q9. What are some common challenges or issues that can occur during backward propagation, and how\n",
        "can they be addressed?"
      ],
      "metadata": {
        "id": "iACRkf6IKB_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers to your questions:\n",
        "\n",
        "Q1: Forward propagation is the process of passing input data through a neural network to generate output predictions.\n",
        "\n",
        "Q2: Mathematically, forward propagation in a single-layer feedforward neural network can be represented as:\n",
        "\n",
        "Output = Activation Function (Weights * Input + Bias)\n",
        "\n",
        "Q3: Activation functions are used to introduce non-linearity into the neural network, enabling it to learn complex relationships between inputs and outputs.\n",
        "\n",
        "Q4: Weights and biases are used to adjust the input data during forward propagation, allowing the neural network to learn patterns and relationships.\n",
        "\n",
        "Q5: The softmax function is applied in the output layer to generate probability distributions over multiple classes, enabling the neural network to make multi-class predictions.\n",
        "\n",
        "Q6: Backward propagation is the process of calculating the error gradient of the loss function with respect to the neural network's parameters, used for training and optimization.\n",
        "\n",
        "Q7: Mathematically, backward propagation in a single-layer feedforward neural network can be represented as:\n",
        "\n",
        "Error Gradient = (Target Output - Predicted Output) * Activation Function Derivative\n",
        "\n",
        "Q8: The chain rule is a mathematical rule used to compute the derivative of a composite function, essential for backward propagation. It enables the calculation of error gradients for each parameter in the neural network.\n",
        "\n",
        "Q9: Common challenges during backward propagation include:\n",
        "\n",
        "- Vanishing or exploding gradients\n",
        "- Numerical instability\n",
        "- Overfitting\n",
        "\n",
        "These can be addressed using techniques such as:\n",
        "\n",
        "- Gradient clipping\n",
        "- Gradient normalization\n",
        "- Regularization\n",
        "- Batch normalization\n",
        "\n",
        "Note: These answers provide a brief overview of the concepts. If you need more detailed explanations or examples, feel free to ask!"
      ],
      "metadata": {
        "id": "wON_CyhpKNEI"
      }
    }
  ]
}