{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
        "can they be mitigated?\n",
        "\n",
        "Q2: How can we reduce overfitting? Explain in brief.\n",
        "\n",
        "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "\n",
        "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
        "variance, and how do they affect model performance?\n",
        "\n",
        "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
        "How can you determine whether your model is overfitting or underfitting?\n",
        "\n",
        "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
        "and high variance models, and how do they differ in terms of their performance?\n",
        "\n",
        "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
        "some common regularization techniques and how they work."
      ],
      "metadata": {
        "id": "S9eiiKr4Y0ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: **Overfitting and Underfitting**:\n",
        "- **Overfitting**: Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations that are specific to the training set and do not generalize well to new, unseen data. Consequences include poor performance on test data, high variance, and lack of generalization.\n",
        "- **Underfitting**: Underfitting occurs when a model is too simple to capture the underlying structure of the data, resulting in poor performance both on the training data and on new data. Consequences include high bias, low variance, and a failure to learn from the training data effectively.\n",
        "\n",
        "**Mitigation**:\n",
        "- **Overfitting**: Regularization techniques like L1 and L2 regularization, early stopping, cross-validation, and reducing model complexity (e.g., feature selection, dimensionality reduction) can help mitigate overfitting.\n",
        "- **Underfitting**: Increasing model complexity (e.g., adding more features, using a more complex model), collecting more training data, or reducing regularization can help mitigate underfitting.\n",
        "\n",
        "Q2: **Reducing Overfitting**:\n",
        "- Regularization techniques: L1 and L2 regularization penalize large model weights, reducing overfitting by preventing the model from fitting the noise in the data.\n",
        "- Cross-validation: Splitting the data into multiple train-test splits and evaluating model performance on each split helps to detect overfitting and select the best-performing model.\n",
        "- Feature selection and dimensionality reduction: Removing irrelevant or redundant features can reduce model complexity and improve generalization performance.\n",
        "\n",
        "Q3: **Underfitting**:\n",
        "- Underfitting occurs when a model is too simple to capture the underlying patterns in the data.\n",
        "- Scenarios where underfitting can occur include using a linear model for data with complex non-linear relationships, using insufficiently expressive models, or when the training data is noisy or insufficient.\n",
        "\n",
        "Q4: **Bias-Variance Tradeoff**:\n",
        "- The bias-variance tradeoff refers to the balance between bias and variance in machine learning models.\n",
        "- **Bias**: Error due to overly simplistic assumptions in the model. High bias models may underfit the data.\n",
        "- **Variance**: Error due to too much complexity in the model. High variance models may overfit the data.\n",
        "- The relationship between bias and variance is inversely proportional: Increasing the bias of a model typically decreases its variance and vice versa.\n",
        "- Balancing bias and variance is essential for achieving good generalization performance.\n",
        "\n",
        "Q5: **Detecting Overfitting and Underfitting**:\n",
        "- Overfitting: Monitoring the model's performance on both the training and test sets, observing large discrepancies between the two, performing cross-validation, and analyzing learning curves.\n",
        "- Underfitting: Similar to detecting overfitting, observing poor performance on both the training and test sets, analyzing learning curves, and examining model complexity.\n",
        "\n",
        "Q6: **Bias vs. Variance**:\n",
        "- **Bias**: Error from overly simplistic assumptions, leading to models that are too rigid and fail to capture the underlying patterns in the data. High bias models typically have low complexity and generalize poorly.\n",
        "- **Variance**: Error from too much complexity, leading to models that are overly flexible and fit the noise in the data. High variance models have high complexity and tend to overfit the training data.\n",
        "\n",
        "**Examples**:\n",
        "- High bias: Linear regression with few features for data with non-linear relationships.\n",
        "- High variance: Deep neural networks with many layers trained on a small dataset.\n",
        "\n",
        "Q7: **Regularization in Machine Learning**:\n",
        "- Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that penalizes large model weights.\n",
        "- Common regularization techniques include L1 and L2 regularization (also known as Lasso and Ridge regression), dropout regularization in neural networks, and early stopping during training.\n",
        "- L1 regularization adds a penalty proportional to the absolute value of the weights, promoting sparsity and feature selection.\n",
        "- L2 regularization adds a penalty proportional to the square of the weights, leading to smoother models and reducing the impact of outliers."
      ],
      "metadata": {
        "id": "PcK6Y7yOZG9d"
      }
    }
  ]
}