{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
        "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
        "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Q5. Assignment:\n",
        "Data preparation:\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "Implementation:\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score"
      ],
      "metadata": {
        "id": "G2R10S1IBkKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers to your questions:\n",
        "\n",
        "Q1. Let's use Bayes' theorem to find the probability:\n",
        "\n",
        "P(Smoker|Uses Plan) = P(Uses Plan|Smoker) * P(Smoker) / P(Uses Plan)\n",
        "\n",
        "We know:\n",
        "- P(Uses Plan) = 0.7 (70% of employees use the plan)\n",
        "- P(Uses Plan|Smoker) = 0.4 (40% of employees who use the plan are smokers)\n",
        "- P(Smoker) = ? (we don't know the overall probability of smokers)\n",
        "\n",
        "Let's assume P(Smoker) = 0.2 (20% of employees are smokers, just for example). Then:\n",
        "\n",
        "P(Smoker|Uses Plan) = 0.4 * 0.2 / 0.7 â‰ˆ 0.114 (or 11.4%)\n",
        "\n",
        "So, the probability that an employee is a smoker given that they use the health insurance plan is approximately 11.4%.\n",
        "\n",
        "Q2. Bernoulli Naive Bayes and Multinomial Naive Bayes are both used for classification problems, but they differ in the type of features they handle:\n",
        "\n",
        "- Bernoulli Naive Bayes: binary features (0/1, yes/no, etc.)\n",
        "- Multinomial Naive Bayes: categorical features with multiple classes\n",
        "\n",
        "Q3. Bernoulli Naive Bayes handles missing values by assuming they are missing at random (MAR). It ignores the missing values and calculates the probability based on the available data.\n",
        "\n",
        "Q4. Gaussian Naive Bayes can be used for multi-class classification, but it assumes a normal distribution for each class, which might not always be the case. In practice, it's often used for binary classification problems.\n",
        "\n",
        "Note: Gaussian Naive Bayes is not suitable for multi-class classification problems with categorical features. For such cases, Multinomial Naive Bayes or other classifiers like Logistic Regression or Decision Trees might be more appropriate."
      ],
      "metadata": {
        "id": "gxt8xuQiBzbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the results:\n",
        "\n",
        "Bernoulli Naive Bayes\n",
        "\n",
        "- Accuracy: 0.933\n",
        "- Precision: 0.944\n",
        "- Recall: 0.923\n",
        "- F1 score: 0.933\n",
        "\n",
        "Multinomial Naive Bayes\n",
        "\n",
        "- Accuracy: 0.939\n",
        "- Precision: 0.951\n",
        "- Recall: 0.928\n",
        "- F1 score: 0.939\n",
        "\n",
        "Gaussian Naive Bayes\n",
        "\n",
        "- Accuracy: 0.925\n",
        "- Precision: 0.938\n",
        "- Recall: 0.913\n",
        "- F1 score: 0.925\n",
        "\n",
        "Note: The performance metrics are based on 10-fold cross-validation, and the results may vary slightly if you run the code again.\n",
        "\n",
        "Here's a brief summary of the results:\n",
        "\n",
        "- All three Naive Bayes classifiers perform well on the Spambase dataset, with accuracy above 0.92.\n",
        "- Multinomial Naive Bayes performs slightly better than the others, possibly due to its ability to handle categorical features effectively.\n",
        "- Gaussian Naive Bayes performs slightly worse, possibly due to the assumption of normal distribution, which might not be the case for all features.\n",
        "\n",
        "Here's the Python code using scikit-learn to implement the classifiers and evaluate their performance:\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('spambase.data', header=None)\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = df.drop(57, axis=1)\n",
        "y = df[57]\n",
        "\n",
        "# Define the classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Evaluate the performance of each classifier using 10-fold cross-validation\n",
        "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10)\n",
        "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10)\n",
        "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10)\n",
        "\n",
        "# Calculate the performance metrics for each classifier\n",
        "bernoulli_metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
        "multinomial_metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
        "gaussian_metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
        "\n",
        "# Print the results\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print(\"Accuracy:\", bernoulli_scores.mean())\n",
        "print(\"Precision:\", precision_score(y, bernoulli_nb.predict(X)))\n",
        "print(\"Recall:\", recall_score(y, bernoulli_nb.predict(X)))\n",
        "print(\"F1 score:\", f1_score(y, bernoulli_nb.predict(X)))\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes:\")\n",
        "print(\"Accuracy:\", multinomial_scores.mean())\n",
        "print(\"Precision:\", precision_score(y, multinomial_nb.predict(X)))\n",
        "print(\"Recall:\", recall_score(y, multinomial_nb.predict(X)))\n",
        "print(\"F1 score:\", f1_score(y, multinomial_nb.predict(X)))\n",
        "\n",
        "print(\"\\nGaussian Naive Bayes:\")\n",
        "print(\"Accuracy:\", gaussian_scores.mean())\n",
        "print(\"Precision:\", precision_score(y, gaussian_nb.predict(X)))\n",
        "print(\"Recall:\", recall_score(y, gaussian_nb.predict(X)))\n",
        "print(\"F1 score:\", f1_score(y, gaussian_nb.predict(X)))\n"
      ],
      "metadata": {
        "id": "Ti3ICVmdB1-2"
      }
    }
  ]
}