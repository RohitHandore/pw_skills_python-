{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?\n",
        "\n",
        "Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
        "which type of problem?\n",
        "\n",
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
        "and how can these be addressed?\n",
        "\n",
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "Q10. What is the role of feature scaling in KNN?"
      ],
      "metadata": {
        "id": "IMY_yhJrvqPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers to your questions:\n",
        "\n",
        "1. The KNN (K-Nearest Neighbors) algorithm is a supervised machine learning algorithm that classifies or predicts the output of a new input based on the majority vote of its K nearest neighbors.\n",
        "\n",
        "2. The value of K is chosen based on the dataset and the problem. A small value of K (e.g., 1-5) is suitable for simple datasets, while a larger value of K (e.g., 10-20) is suitable for more complex datasets. Cross-validation can be used to determine the optimal value of K.\n",
        "\n",
        "3. The KNN classifier predicts a class label, while the KNN regressor predicts a continuous value.\n",
        "\n",
        "4. The performance of KNN can be measured using metrics such as accuracy, precision, recall, F1-score, mean squared error (MSE), and mean absolute error (MAE).\n",
        "\n",
        "5. The curse of dimensionality in KNN refers to the phenomenon where the algorithm becomes less efficient and accurate as the number of features increases. This is because the distance between points in high-dimensional space becomes less meaningful.\n",
        "\n",
        "6. Missing values can be handled in KNN by ignoring them, imputing them with mean or median values, or using a different distance metric that can handle missing values.\n",
        "\n",
        "7. The KNN classifier is suitable for classification problems with a small number of classes, while the KNN regressor is suitable for regression problems with continuous outputs. The classifier is generally faster and more accurate, while the regressor is more flexible and can handle non-linear relationships.\n",
        "\n",
        "8. Strengths of KNN include its simplicity, flexibility, and ability to handle non-linear relationships. Weaknesses include its sensitivity to the choice of K, the curse of dimensionality, and its computational complexity. These can be addressed by using techniques such as cross-validation, feature selection, and dimensionality reduction.\n",
        "\n",
        "9. Euclidean distance is a straight-line distance between two points, while Manhattan distance is the sum of the absolute differences between corresponding features. Euclidean distance is more sensitive to outliers, while Manhattan distance is more robust.\n",
        "\n",
        "10. Feature scaling is important in KNN to ensure that all features are treated equally, regardless of their scales. This can be achieved using techniques such as normalization or standardization.\n",
        "\n",
        "Note: KNN is a simple yet powerful algorithm that can be used for both classification and regression tasks. However, its performance can be affected by the choice of K, the curse of dimensionality, and missing values. By understanding these limitations and using appropriate techniques, KNN can be a effective tool for machine learning tasks."
      ],
      "metadata": {
        "id": "GC5Th6frvxFq"
      }
    }
  ]
}