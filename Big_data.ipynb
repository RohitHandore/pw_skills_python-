{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and\n",
        "storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
        "\n",
        "2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a\n",
        "distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and\n",
        "how they contribute to data reliability and fault tolerance.\n",
        "\n",
        "3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to\n",
        "illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing\n",
        "large datasets.\n",
        "\n",
        "4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n",
        "Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
        "\n",
        "5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig,\n",
        "and Spark. Describe the use cases and differences between these components. Choose one component and\n",
        "explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
        "\n",
        "6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome\n",
        "some of the limitations of MapReduce for big data processing tasks?\n",
        "\n",
        "7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word,\n",
        "and returns the top 10 most frequent words. Explain the key components and steps involved in this\n",
        "application.\n",
        "\n",
        "8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your\n",
        "choice:\n",
        "a. Filter the data to select only rows that meet specific criteria.\n",
        "b. Map a transformation to modify a specific column in the dataset.\n",
        "c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average).\n",
        "\n",
        "9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the\n",
        "following operations:\n",
        "a. Select specific columns from the DataFrame.\n",
        "b. Filter rows based on certain conditions.\n",
        "c. Group the data by a particular column and calculate aggregations (e.g., sum, average).\n",
        "d. Join two DataFrames based on a common key."
      ],
      "metadata": {
        "id": "QFONiYoSdIu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Core Components of the Hadoop Ecosystem**:\n",
        "\n",
        "   - **Hadoop Distributed File System (HDFS)**: A distributed file system designed to store vast amounts of data across multiple machines in a Hadoop cluster. It provides high-throughput access to data and offers fault tolerance.\n",
        "   \n",
        "   - **MapReduce**: A programming model and processing engine for distributed computing, specifically designed for processing large datasets in parallel across a Hadoop cluster.\n",
        "   \n",
        "   - **YARN (Yet Another Resource Negotiator)**: A resource management layer responsible for managing resources in a Hadoop cluster and scheduling tasks. It decouples the resource management and job scheduling functionalities of Hadoop, allowing different processing frameworks to run on the same cluster.\n",
        "   \n",
        "   - **Hadoop Common**: A set of utilities and libraries used by other Hadoop modules. It includes common utilities, such as authentication and configuration.\n",
        "   \n",
        "   - **Hadoop MapReduce Libraries**: A collection of libraries and tools for building MapReduce applications.\n",
        "   \n",
        "   - **Hadoop Ozone (Object Store)**: A scalable, distributed object store designed for managing petabytes of unstructured data.\n",
        "   \n",
        "2. **Hadoop Distributed File System (HDFS)**:\n",
        "\n",
        "   - **Storage and Management**: HDFS stores data across a distributed cluster of commodity hardware. It provides fault tolerance by replicating data across multiple nodes in the cluster.\n",
        "   \n",
        "   - **NameNode**: Acts as the master node in the HDFS architecture. It manages the metadata of the file system, such as the directory tree and namespace, and coordinates access to data stored in DataNodes.\n",
        "   \n",
        "   - **DataNode**: Acts as the slave node in the HDFS architecture. DataNodes store actual data blocks and respond to read and write requests from clients.\n",
        "   \n",
        "   - **Blocks**: HDFS breaks files into smaller blocks (typically 128 MB or 256 MB) and distributes these blocks across multiple DataNodes in the cluster. Replication ensures data reliability and fault tolerance.\n",
        "   \n",
        "   - **Replication**: HDFS replicates each block multiple times (usually three times by default) across different DataNodes to ensure data reliability. If a DataNode fails, the NameNode can identify the missing replicas and replicate them from other DataNodes.\n",
        "\n",
        "3. **MapReduce Framework**:\n",
        "\n",
        "   - **Map Phase**: In the Map phase, input data is divided into smaller chunks and processed independently by multiple mapper tasks. Each mapper applies a user-defined function (map function) to transform the input data into intermediate key-value pairs.\n",
        "   \n",
        "   - **Shuffle and Sort**: Intermediate key-value pairs produced by mappers are sorted and grouped by keys before being sent to reducer tasks. This phase ensures that all values associated with the same key are processed together by the same reducer.\n",
        "   \n",
        "   - **Reduce Phase**: In the Reduce phase, reducer tasks process the intermediate key-value pairs generated by mappers. Reducers apply a user-defined function (reduce function) to aggregate, summarize, or analyze the data, producing the final output.\n",
        "   \n",
        "   - **Example**: Suppose we have a large dataset of website access logs and want to count the number of visits for each URL. In the Map phase, mappers process log entries and emit key-value pairs with the URL as the key and 1 as the value. In the Reduce phase, reducers receive these intermediate key-value pairs, group them by URL, and sum up the counts to get the total number of visits for each URL.\n",
        "   \n",
        "   - **Advantages**: MapReduce provides fault tolerance, scalability, and the ability to process large datasets efficiently by distributing the workload across multiple nodes in a cluster. It is well-suited for batch processing tasks.\n",
        "   \n",
        "   - **Limitations**: MapReduce may not be the most efficient framework for iterative algorithms or real-time processing tasks due to its disk-based shuffle and sort operations, which can incur high I/O overhead. Additionally, writing MapReduce programs often requires low-level programming and may not be as intuitive as working with higher-level abstractions provided by other frameworks like Apache Spark."
      ],
      "metadata": {
        "id": "-UFL-wSldXZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Role of YARN in Hadoop**:\n",
        "\n",
        "   - **Resource Management**: YARN (Yet Another Resource Negotiator) is responsible for managing resources in a Hadoop cluster. It allocates resources to various applications and schedules tasks to run on different nodes in the cluster.\n",
        "   \n",
        "   - **Resource Types**: YARN manages both CPU and memory resources in the cluster. It allows different types of applications, such as MapReduce, Spark, and Tez, to coexist and share cluster resources efficiently.\n",
        "   \n",
        "   - **Decoupling of Resource Management and Job Scheduling**: Unlike the earlier Hadoop 1.x architecture, where the JobTracker was responsible for both resource management and job scheduling, YARN separates these functionalities. This decoupling allows for better resource utilization and enables multiple processing frameworks to run concurrently on the same cluster.\n",
        "   \n",
        "   - **Scheduler**: YARN includes multiple schedulers, such as the CapacityScheduler and the FairScheduler, which allow users to define policies for resource allocation and job prioritization based on their specific requirements.\n",
        "   \n",
        "   - **Scalability and Flexibility**: YARN provides scalability and flexibility by allowing clusters to scale dynamically and support a wide range of processing frameworks and applications beyond MapReduce.\n",
        "\n",
        "5. **Popular Components in the Hadoop Ecosystem**:\n",
        "\n",
        "   - **HBase**: A NoSQL distributed database designed for real-time read/write access to large datasets. It is suitable for applications requiring low-latency data access, such as real-time analytics and operational databases.\n",
        "   \n",
        "   - **Hive**: A data warehouse infrastructure built on top of Hadoop that provides a SQL-like interface for querying and analyzing data stored in Hadoop. Hive is commonly used for batch processing and data warehousing tasks, such as ETL (Extract, Transform, Load) operations and ad-hoc querying.\n",
        "   \n",
        "   - **Pig**: A high-level data flow language and execution framework for processing and analyzing large datasets. Pig simplifies data processing tasks by providing a scripting language (Pig Latin) that abstracts complex MapReduce operations into simple data transformations.\n",
        "   \n",
        "   - **Spark**: A fast and general-purpose cluster computing framework that provides in-memory data processing capabilities. Spark offers a rich set of libraries for batch processing, interactive querying, machine learning, and stream processing. It is known for its speed, ease of use, and support for complex analytics workflows.\n",
        "\n",
        "   **Integration into Hadoop Ecosystem**: Let's take Spark as an example. Spark can be integrated into the Hadoop ecosystem by running Spark applications on YARN, which allows Spark to leverage the distributed computing capabilities of the Hadoop cluster. Spark can also read and write data from/to HDFS and other storage systems compatible with Hadoop, enabling seamless integration with existing data pipelines and workflows.\n",
        "\n",
        "6. **Differences Between Apache Spark and Hadoop MapReduce**:\n",
        "\n",
        "   - **Processing Model**: Spark uses an in-memory processing model, whereas Hadoop MapReduce relies heavily on disk-based processing. This allows Spark to achieve significantly faster performance for iterative algorithms and interactive analytics.\n",
        "   \n",
        "   - **Ease of Use**: Spark provides a more developer-friendly API and supports higher-level abstractions, such as DataFrames and Datasets, making it easier to write complex data processing tasks compared to the low-level programming required for MapReduce.\n",
        "   \n",
        "   - **Advanced Analytics**: Spark includes built-in libraries for machine learning (MLlib) and stream processing (Spark Streaming), whereas Hadoop MapReduce primarily focuses on batch processing. This allows Spark to support a wider range of big data processing tasks out of the box.\n",
        "   \n",
        "   - **Fault Tolerance**: Both Spark and Hadoop MapReduce provide fault tolerance by recomputing lost data or tasks, but Spark achieves this more efficiently by keeping track of the lineage of resilient distributed datasets (RDDs) and using lineage-based recovery mechanisms.\n",
        "\n",
        "   **Advantages of Spark over MapReduce**: Spark's in-memory processing, higher-level abstractions, and rich set of libraries make it well-suited for a wide range of big data processing tasks, including iterative algorithms, interactive analytics, and real-time processing. Spark's performance, ease of use, and support for advanced analytics workflows make it a popular choice for organizations looking to leverage big data for insights and decision-making."
      ],
      "metadata": {
        "id": "JLvwiyQ0dm03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Spark Application for Word Count**:\n",
        "\n",
        "```python\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark configuration\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local[*]\")\n",
        "\n",
        "# Initialize Spark context\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Read input text file\n",
        "lines = sc.textFile(\"input.txt\")\n",
        "\n",
        "# Split each line into words\n",
        "words = lines.flatMap(lambda line: line.split())\n",
        "\n",
        "# Map each word to a tuple (word, 1)\n",
        "word_counts = words.map(lambda word: (word, 1))\n",
        "\n",
        "# Reduce by key to count occurrences of each word\n",
        "word_counts = word_counts.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Sort the word counts in descending order\n",
        "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Take top 10 most frequent words\n",
        "top_10_words = sorted_word_counts.take(10)\n",
        "\n",
        "# Print the top 10 words\n",
        "for word, count in top_10_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Stop Spark context\n",
        "sc.stop()\n",
        "```\n",
        "\n",
        "**Key Components and Steps**:\n",
        "- **Spark Context**: Initialize a Spark context to connect to the Spark cluster.\n",
        "- **Read Input**: Read the input text file (`input.txt`) as an RDD (Resilient Distributed Dataset).\n",
        "- **Split Words**: Split each line into words using `flatMap`.\n",
        "- **Map and Reduce**: Map each word to a tuple `(word, 1)` and then reduce by key to count occurrences of each word.\n",
        "- **Sort**: Sort the word counts in descending order based on the count.\n",
        "- **Take Top 10**: Take the top 10 most frequent words.\n",
        "- **Print Results**: Print the top 10 words and their counts.\n",
        "- **Stop Spark Context**: Stop the Spark context to release resources.\n",
        "\n",
        "8. **Tasks Using Spark RDDs**:\n",
        "\n",
        "```python\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Create Spark configuration\n",
        "conf = SparkConf().setAppName(\"RDD Tasks\").setMaster(\"local[*]\")\n",
        "\n",
        "# Initialize Spark context\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Load dataset into RDD\n",
        "data = sc.parallelize([(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35), (4, \"David\", 40), (5, \"Eve\", 45)])\n",
        "\n",
        "# a. Filter data to select only rows with age > 30\n",
        "filtered_data = data.filter(lambda row: row[2] > 30)\n",
        "\n",
        "# b. Map transformation to double the age\n",
        "mapped_data = data.map(lambda row: (row[0], row[1], row[2] * 2))\n",
        "\n",
        "# c. Reduce dataset to calculate sum of ages\n",
        "total_age = data.map(lambda row: row[2]).reduce(lambda x, y: x + y)\n",
        "\n",
        "print(\"Filtered Data:\")\n",
        "print(filtered_data.collect())\n",
        "\n",
        "print(\"Mapped Data:\")\n",
        "print(mapped_data.collect())\n",
        "\n",
        "print(\"Total Age:\", total_age)\n",
        "\n",
        "# Stop Spark context\n",
        "sc.stop()\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- **Filter**: Use the `filter` transformation to select only rows where the age is greater than 30.\n",
        "- **Map**: Apply a transformation using `map` to double the age for each row.\n",
        "- **Reduce**: Use the `reduce` action to calculate the sum of ages across all rows.\n",
        "\n",
        "9. **Spark DataFrame Operations**:\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Operations\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load dataset into DataFrame\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# a. Select specific columns\n",
        "selected_cols = df.select(\"Name\", \"Age\")\n",
        "\n",
        "# b. Filter rows based on condition (age > 30)\n",
        "filtered_df = df.filter(df[\"Age\"] > 30)\n",
        "\n",
        "# c. Group by column (Gender) and calculate average age\n",
        "avg_age_by_gender = df.groupBy(\"Gender\").avg(\"Age\")\n",
        "\n",
        "# d. Join two DataFrames based on common key (ID)\n",
        "df1 = df.select(\"ID\", \"Name\")\n",
        "df2 = df.select(\"ID\", \"Salary\")\n",
        "joined_df = df1.join(df2, \"ID\", \"inner\")\n",
        "\n",
        "# Show results\n",
        "print(\"Selected Columns:\")\n",
        "selected_cols.show()\n",
        "\n",
        "print(\"Filtered DataFrame:\")\n",
        "filtered_df.show()\n",
        "\n",
        "print(\"Average Age by Gender:\")\n",
        "avg_age_by_gender.show()\n",
        "\n",
        "print(\"Joined DataFrame:\")\n",
        "joined_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "**Explanation**:\n",
        "- **Load Data**: Load the dataset (CSV file) into a DataFrame.\n",
        "- **Select Columns**: Use `select` to choose specific columns from the DataFrame.\n",
        "- **Filter**: Apply a filter condition to keep rows where age is greater than 30.\n",
        "- **Group By**: Group the DataFrame by a column (e.g., Gender) and calculate aggregations like average age.\n",
        "- **Join**: Join two DataFrames based on a common key (e.g., ID) using the `join` operation."
      ],
      "metadata": {
        "id": "0UmnK1hTd0OQ"
      }
    }
  ]
}