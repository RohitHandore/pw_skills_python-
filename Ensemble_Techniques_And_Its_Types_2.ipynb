{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. How does bagging reduce overfitting in decision trees?\n",
        "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
        "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
        "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
        "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
        "Q6. Can you provide an example of a real-world application of bagging in machine learning?"
      ],
      "metadata": {
        "id": "ELPf1Fi90Yip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the answers to your questions ¹ ² ³:\n",
        "\n",
        "1. Bagging reduces overfitting by introducing diversity among base learners and improving overall performance by reducing variance and increasing robustness. It entails generating numerous subsets of the training data by employing random sampling with replacement.\n",
        "\n",
        "2. Advantages of bagging include its ability to reduce variance, combat overfitting, be less sensitive to outliers and noisy data points, be versatile and relatively straightforward. The primary disadvantage is the increased computational cost, but this can be mitigated by parallelizing the training of individual base learners.\n",
        "\n",
        "3. The choice of base learner affects the bias-variance tradeoff in bagging, as different base learners may have varying levels of bias and variance. For instance, decision trees tend to have high variance and low bias, while neural networks may have high bias and low variance.\n",
        "\n",
        "4. Bagging can be used for both classification and regression tasks. For classification tasks, the final prediction is determined by majority voting, while for regression tasks, the final prediction is obtained by averaging the predictions.\n",
        "\n",
        "5. The role of ensemble size in bagging is to control the tradeoff between bias and variance. A larger ensemble size can reduce variance but may increase bias, while a smaller ensemble size can reduce bias but may increase variance. The ideal ensemble size depends on the specific problem and data.\n",
        "\n",
        "6. An example of a real-world application of bagging is in image classification. For instance, bagging can be used to combine the predictions of multiple models, such as logistic regression, decision trees, and support vector machines, to improve the accuracy of image classification."
      ],
      "metadata": {
        "id": "1iijNzcl0ejE"
      }
    }
  ]
}